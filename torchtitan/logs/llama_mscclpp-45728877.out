Starting Training with MSCCL++ (Job ID: 45728877)...
7: [rank7]: Traceback (most recent call last):
7: [rank7]:   File "/pscratch/sd/x/xz987/CS5470/final_project/torchtitan/bench_comm.py", line 77, in <module>
7: [rank7]:     main()
7: [rank7]:   File "/pscratch/sd/x/xz987/CS5470/final_project/torchtitan/bench_comm.py", line 42, in main
7: [rank7]:     torch.cuda.set_device(rank)
7: [rank7]:   File "/pscratch/sd/x/xz987/conda_storage/.conda/envs/ccl-bench/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
7: [rank7]:     torch._C._cuda_setDevice(device)
7: [rank7]: torch.AcceleratorError: CUDA error: invalid device ordinal
7: [rank7]: GPU device may be out of range, do you have enough GPUs?
7: [rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
7: [rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
7: [rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
7: 
5: [rank5]: Traceback (most recent call last):
5: [rank5]:   File "/pscratch/sd/x/xz987/CS5470/final_project/torchtitan/bench_comm.py", line 77, in <module>
5: [rank5]:     main()
5: [rank5]:   File "/pscratch/sd/x/xz987/CS5470/final_project/torchtitan/bench_comm.py", line 42, in main
5: [rank5]:     torch.cuda.set_device(rank)
5: [rank5]:   File "/pscratch/sd/x/xz987/conda_storage/.conda/envs/ccl-bench/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
5: [rank5]:     torch._C._cuda_setDevice(device)
5: [rank5]: torch.AcceleratorError: CUDA error: invalid device ordinal
5: [rank5]: GPU device may be out of range, do you have enough GPUs?
5: [rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
5: [rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
5: [rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
5: 
6: [rank6]: Traceback (most recent call last):
6: [rank6]:   File "/pscratch/sd/x/xz987/CS5470/final_project/torchtitan/bench_comm.py", line 77, in <module>
6: [rank6]:     main()
6: [rank6]:   File "/pscratch/sd/x/xz987/CS5470/final_project/torchtitan/bench_comm.py", line 42, in main
6: [rank6]:     torch.cuda.set_device(rank)
6: [rank6]:   File "/pscratch/sd/x/xz987/conda_storage/.conda/envs/ccl-bench/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
6: [rank6]:     torch._C._cuda_setDevice(device)
6: [rank6]: torch.AcceleratorError: CUDA error: invalid device ordinal
6: [rank6]: GPU device may be out of range, do you have enough GPUs?
6: [rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
6: [rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
6: [rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
6: 
4: [rank4]: Traceback (most recent call last):
4: [rank4]:   File "/pscratch/sd/x/xz987/CS5470/final_project/torchtitan/bench_comm.py", line 77, in <module>
4: [rank4]:     main()
4: [rank4]:   File "/pscratch/sd/x/xz987/CS5470/final_project/torchtitan/bench_comm.py", line 42, in main
4: [rank4]:     torch.cuda.set_device(rank)
4: [rank4]:   File "/pscratch/sd/x/xz987/conda_storage/.conda/envs/ccl-bench/lib/python3.10/site-packages/torch/cuda/__init__.py", line 567, in set_device
4: [rank4]:     torch._C._cuda_setDevice(device)
4: [rank4]: torch.AcceleratorError: CUDA error: invalid device ordinal
4: [rank4]: GPU device may be out of range, do you have enough GPUs?
4: [rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
4: [rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
4: [rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
4: 
4: [rank4]:[W1126 19:42:00.647155611 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
srun: error: nid002044: tasks 4-6: Exited with exit code 1
srun: Terminating StepId=45728877.0
0: slurmstepd: error: *** STEP 45728877.0 ON nid001669 CANCELLED AT 2025-11-27T03:42:03 ***
srun: error: nid002044: task 7: Exited with exit code 1
srun: error: nid001669: tasks 0-3: Terminated
srun: Force Terminated StepId=45728877.0
