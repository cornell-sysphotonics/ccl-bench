# Tool Development

Tool development: Byungsoo, Jamal (wasn't sure we we need to edit)

Metric collection: Byungsoo, Jinkun

## Pipeline

1. Move target trace to `ccl-bench/trace_collection/<trace_name>`

   Example: `ccl-bench/trace_collection/llama-3.18b-fsdp_4-torchprime_xla_tpu-group-21`

2. Define metrics

   Metrics should always include a number (integer, float) that could be presented on the benchmark.
   Other metric formats could be collected in addition, such as distributions or time series.

   Examples of metrics we support:

   - Time-based metrics: wall time, compute time, communication time
   - Utilization metrics: compute utilization proxy, communication fraction
   - Performance metrics: bandwidth, throughput, FLOPs
   - Model parameters: Hockney model alpha/beta for communication modeling

   See the complete list of supported metrics in the "Metrics" section below.

3. Develop tools
   ```
   Input: Chrome trace JSON files (.json or .json.gz) stored in the trace directory
   Output: float | int
   ```
4. Define tool-trace mapping

   The main script can be used on all trace files generated by Torch XLA, that have the format of
   (zipped) trace_name.trace.json
   trace_name.xplan.pb

   The only thing that needs to be changed if you increase the number of TPU chips is the parameter, --n_chips
   in the function call.

5. Calculate metrics

   Basic usage:

   ```
   python main.py --trace=<trace directory> --metric=<name of metric>
   ```

   Examples:

   ```bash
   # Simple metrics (no extra parameters)
   python main.py --trace ./trace_collection/my_trace --metric wall_time_s
   python main.py --trace ./trace_collection/my_trace --metric total_compute_time_s

   # Metrics requiring number of chips
   python main.py --trace ./trace_collection/my_trace --metric avg_comm_bandwidth_GBps --n_chips 4
   python main.py --trace ./trace_collection/my_trace --metric hockney_alpha_s --n_chips 4

   # Throughput metrics (optional model_params)
   python main.py --trace ./trace_collection/my_trace --metric throughput
   python main.py --trace ./trace_collection/my_trace --metric estimated_throughput_tokens_per_s --model_params 7e9
   ```

   Or use scripts:

   ```
   ./scripts/get_<name of metric>.sh
   ```

#Note: not all of these metrics have their own script, they are all calculated in some degree and are included in our original metrics script. This was orginally in a collab notebook so it would be easier to share and collaborate on the code, so we had to split it up after

## Metrics

**Note:** Metrics marked with _N_CHIPS_ require the `--n_chips` parameter. Metrics marked with _M_PARAMS_ can optionally use `--model_params` for more accurate throughput estimation.

1. `wall_time_s`: total elapsed wall-clock time covered by the trace

2. `total_compute_time_s`: total time spent in compute operations during the trace

3. `total_comm_time_s`: total time spent in communication operations during the trace

4. `avg_comm_kernel_time_s`: average execution time of a single communication kernel

5. `compute_utilization_proxy`: fraction of total wall-clock time spent in computation

6. `communication_fraction`: fraction of total wall-clock time spent in communication

7. `num_comm_kernels` _N_CHIP_: number of communication kernels executed in the trace (requires `--n_chips`)

8. `num_comm_kernels_w_size_and_time` _N_CHIP_: number of communication kernels with valid message size and execution time (requires `--n_chips`)

9. `avg_comm_bandwidth_GBps` _N_CHIP_: average achieved communication bandwidth across valid kernels (GB/s) (requires `--n_chips`)

10. `allreduce_comm_time_s`: total time spent in AllReduce communication operations

11. `hockney_alpha_s` _N_CHIP_: latency term (α) from fitting the AllReduce Hockney communication model (requires `--n_chips`)

12. `hockney_beta_s_per_byte` _N_CHIP_: bandwidth cost term (β) from the AllReduce Hockney model (requires `--n_chips`)

13. `hockney_inverse_beta_Bps` _N_CHIP_: inverse of β, representing effective communication bandwidth (bytes/sec) (requires `--n_chips`)

14. `achieved_flops_from_trace_json`: achieved model FLOPs per second estimated from trace events

15. `total_model_flops_from_args`: total model FLOPs summed from trace event arguments

16. `flops_per_token_used` _M_PARAMS_: FLOPs per token value used for throughput estimation (optional `--model_params`)

17. `estimated_total_tokens` _M_PARAMS_: estimated number of tokens processed during the trace (optional `--model_params`)

18. `estimated_throughput_tokens_per_s` _M_PARAMS_: estimated throughput measured in tokens per second (optional `--model_params`)

...
