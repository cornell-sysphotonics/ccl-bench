experiment_id: E1.1
description: Llama-8B baseline on 1 GPU (TP=1, PP=1, EP=1)
model:
  name: meta-llama/Llama-3.1-8B
  precision: bfloat16
parallelism:
  tp: 1
  pp: 1
  dp_replicate: 1
  dp_shard: 1
  ep: 1
data:
  batch_size: 4
  seq_len: 2048
  max_tokens: 512
warmup_iterations: 2
profile_iterations: 5
output_dir: trace_collection/llama-8b-tp1
