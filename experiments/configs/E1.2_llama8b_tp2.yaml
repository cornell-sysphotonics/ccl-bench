experiment_id: E1.2
description: Llama-8B tensor parallelism TP=2 on 2 GPUs
model:
  name: meta-llama/Llama-3.1-8B
  precision: bfloat16
parallelism:
  tp: 2
  pp: 1
  dp_replicate: 1
  dp_shard: 1
  ep: 1
data:
  batch_size: 4
  seq_len: 2048
  max_tokens: 512
warmup_iterations: 2
profile_iterations: 5
output_dir: trace_collection/llama-8b-tp2
