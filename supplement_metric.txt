CS5470 – Communication Metrics Specification (DeepSpeed + nsys)

Assume we have, for each iteration and each GPU, a list of communication events from nsys trace.
Each event e has:
- e.type: {AllReduce, AllGather, ReduceScatter, AllToAll, SendRecv, …}
- e.start_time, e.end_time  (in microseconds or milliseconds)
- e.bytes: total payload bytes transferred by this CUDA kernel (from nsys / NCCL info)
- e.stream_id
- e.rank: global rank of the GPU
- e.parallelism_tag: {DP, TP, PP, EP, OTHER}
  (this tag can be inferred from NCCL communicator id / group size + our config,
   or pre-generated by a small mapping script).

We also have, per GPU:
- hardware_link_bandwidth_bytes_per_sec  (e.g., 400 Gbit/s → 50e9 bytes/s)
- iteration_start_time, iteration_end_time

======================================================================
METRIC 0 – Basic Iteration Statistics
======================================================================

Name: iteration_time_mean
Definition:
  Mean wall-clock duration of one training iteration.
Formula:
  iteration_time(i) = iteration_end_time(i) - iteration_start_time(i)
  iteration_time_mean = average over all iterations i.

Name: iteration_time_p99
Definition:
  99th percentile of iteration_time over all iterations.

(These two只是基础背景，方便后面关联通信开销对 iteration 的影响。)

======================================================================
METRIC 1 – Time Breakdown by Category
======================================================================

Categories (for each iteration i and rank r):
- compute_time
- comm_DP_time
- comm_TP_time
- comm_PP_time
- comm_EP_time
- idle_time   (= iteration_time - compute_time - all_comm_time)

Assumptions:
- Any interval covered by a communication event is counted as communication.
- Compute_time 可以近似为：总 iteration_time 减去所有通信事件在该 GPU 上的 union 时间。
  若需要更精细，可以从 nsys 中读取纯 compute kernels 的时间总和。

For each rank r, each iteration i:

1. total_comm_time_DP(i, r) =
   sum over events e with e.parallelism_tag=DP of (e.end_time - e.start_time)

2. 同理：
   total_comm_time_TP, total_comm_time_PP, total_comm_time_EP

3. all_comm_time(i, r) =
   total_comm_time_DP + TP + PP + EP

4. compute_time(i, r) =
   iteration_time(i, r) - union_of_all_comm_intervals(i, r)
   (若不好算 union，可先用简单近似：iteration_time - all_comm_time)

5. idle_time(i, r) =
   iteration_time(i, r) - compute_time(i, r) - all_comm_time(i, r)

Aggregation:
- 对所有 rank 求平均：
  comm_DP_ratio = (sum_i,r total_comm_time_DP(i,r)) / (sum_i,r iteration_time(i,r))
  其他同理。
- 把这些 ratio 画成 stacked bar / pie chart，即可得到时间分解图。

======================================================================
METRIC 2 – Parallelism-Phase Window Time (t3 - t2, t4 - t3)
======================================================================

Goal:
  度量不同并行维度之间的空档（窗口），反映调度和 overlap 情况。

Definition:
  对于一次训练 iteration 中，同一个 rank r 上，存在两类“通信 phase”：
    P1: 一组连续的通信事件，parallelism_tag = X（例如 TP）
    P2: 紧接着的一组通信事件，parallelism_tag = Y（例如 DP）

  对每一对相邻 phase (P1, P2)，定义 window_time:

    window(P1→P2, r) = start_time(P2, r) - end_time(P1, r)

  其中:
    end_time(P1, r) = max_end_time over all events e in P1 on rank r
    start_time(P2, r) = min_start_time over all events e in P2 on rank r

Interpretation:
- window > 0: GPU 在这段时间处于 idle（或只在算别的东西），通信之间存在空洞。
- window < 0: 说明两个 parallelism 的通信 events 在时间上 overlap（部分重叠）。

Implementation sketch:
1. 对每个 rank r，以及每次 iteration i:
   - 把所有通信事件按开始时间排序。
   - 根据 parallelism_tag 把连续同 tag 的事件合并为一个“phase”:
       phase_k = (tag_k, start_k, end_k)
   - 对所有相邻 phase_k, phase_{k+1} 计算窗口：
       window_k = start_{k+1} - end_k

2. 对所有 window 做统计:
   - 按 (tag_k, tag_{k+1}) 分类，比如 (TP→DP), (DP→PP) 等。
   - 对每类计算:
       mean_window
       p50_window
       p95_window
   - 也可以输出所有窗口的 CDF。

======================================================================
METRIC 3 – Per-Event Bandwidth and Bandwidth Utilization (Scheme A)
======================================================================

Goal:
  对每个通信事件，从 nsys trace 直接计算实际带宽，并归一化为硬件利用率；
  然后汇总到不同 parallelism 上。

For each communication event e:
- duration_e = e.end_time - e.start_time   (seconds)
- bytes_e = e.bytes                        (bytes)

Measured bandwidth:
- bw_e = bytes_e / duration_e              (bytes/second)

Hardware bandwidth (per GPU or per link):
- B_hw = hardware_link_bandwidth_bytes_per_sec
  (例如 400Gbps = 400e9 / 8 = 50e9 bytes/s)

Per-event utilization:
- util_e = bw_e / B_hw   (in [0, +∞), often <= 1, e.g. 0.13 = 13%)

Then aggregate:

1. Per parallelism type, per rank:
   For tag ∈ {DP, TP, PP, EP}:
   - avg_bw(tag)      = average_e∈tag (bw_e)
   - avg_util(tag)    = average_e∈tag (util_e)
   - p95_util(tag)    = 95th percentile of util_e over all e with this tag
   - total_bytes(tag) = sum_e∈tag bytes_e

2. Per iteration:
   - 可以算 iteration-level util:
     iteration_bw(i, r) = (sum_e∈iteration bytes_e) / (union_of_comm_time(i, r))
     iteration_util(i, r) = iteration_bw(i, r) / B_hw

3. Over all ranks and iterations:
   - 输出每种 parallelism 的平均带宽利用率：
     global_avg_util(tag) =
       (sum_e∈tag bytes_e / sum_e∈tag duration_e) / B_hw

Notes:
- 如果同一 GPU 有多条物理链路，可以把 B_hw 配置成 “每 GPU 理论最大总带宽”。
- 若需要 per-link 利用率，需要额外从 nsys 中区分是哪个 NIC / NVLink。

======================================================================
METRIC 4 – Communication Volume per Parallelism
======================================================================

Definition:
  对每种 parallelism，统计总通信字节量和事件数。

For each tag ∈ {DP, TP, PP, EP}:
- comm_bytes(tag) = sum_e∈tag bytes_e
- comm_events(tag) = number of events e with this tag

Optionally normalized:
- bytes_per_iteration(tag) = comm_bytes(tag) / num_iterations
- bytes_per_step_per_gpu(tag) = comm_bytes(tag) / (num_iterations * num_ranks)

======================================================================
METRIC 5 – Overlap Ratio (Compute–Communication Overlap)
======================================================================

Goal:
  粗略衡量通信和计算的 overlap 程度。

For each rank r, each iteration i:
- comm_time_union(i, r): union of all communication intervals
- compute_time_union(i, r): union of所有纯 compute kernel 的时间

Define:
- overlapped_time(i, r) = time where communication and compute intervals overlap
- overlap_ratio(i, r) = overlapped_time(i, r) / comm_time_union(i, r)

Aggregate:
- average_overlap_ratio = average over all i, r of overlap_ratio(i, r)

Higher overlap_ratio → 通信隐藏得更好，对 iteration_time 影响更小。

======================================================================
Output Summary (what the script should roughly produce)
======================================================================

For each run / configuration (model + TP/PP/DP/EP settings), the analysis script should output:

1. Basic iteration stats:
   - iteration_time_mean, iteration_time_p99

2. Time breakdown:
   - ratio of compute / comm_DP / comm_TP / comm_PP / comm_EP / idle

3. Window metrics:
   - For each phase pair (X → Y):
       mean_window, p50_window, p95_window, number_of_windows

4. Bandwidth utilization:
   - For each parallelism tag:
       global_avg_util(tag), p95_util(tag), avg_bw(tag), total_bytes(tag)

5. Communication volume:
   - bytes_per_iteration(tag), comm_events(tag)

6. Overlap ratio:
   - average_overlap_ratio (optionally also per parallelism tag if we separate intervals)
