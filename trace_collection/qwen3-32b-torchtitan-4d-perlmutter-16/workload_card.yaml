---
# workload_card.yaml - Qwen3-32B 4D Parallelism Configuration
# Workload: qwen3-32b-torchtitan-4d-perlmutter-16
version: 1

description: >
  Qwen3-32B training on 8 GPUs with 4D parallelism (FSDP+TP+PP+EP).
  Two Perlmutter GPU nodes with 4x NVIDIA A100 (40GB) GPUs each.
  Configuration: dp_shard=2, tp=2, pp=2, ep=4.
  Full 4D parallelism combining data sharding, tensor parallelism,
  pipeline parallelism, and expert parallel for MoE layers.
  Exercises both scale-up (NVLink) and scale-out (Slingshot) paths.
  TorchTitan framework with NCCL;
  uses reduce-scatter/all-gather (FSDP), all-reduce (TP),
  send/recv (PP), and all-to-all (EP).

hf_url: https://huggingface.co/Qwen/Qwen3-32B
trace_url:  # TODO: Add Cornell Box link after uploading traces

workload:
  model:
    phase: training
    moe: true
    granularity: model_fwd_bwd_pass
    model_family: qwen3-32b
    precision: bf16
    epochs: 1
    iteration: 80
  data:
    batch_size: 2
    seq_len: 2048
    dataset: c4
  hardware:
    network_topo:
      topology: slingshot
      bandwidth_gbps:
        - 200  # Scale-out: 200 Gbps per NIC over Slingshot (inter-node)
        - 2000  # Scale-up: Approx NVLink/NVSwitch aggregate bandwidth (Gbps)
    xpu_spec:
      type: GPU
      model: nvidia_a100
      total_count: 8
      count_per_node: 4
    driver_version: cuda_12.4

Model-executor:
  framework:
    name: torchtitan
    compiler_tool_selection: plain_pytorch
  model_plan_parallelization:
    dp_replicate: 1
    dp_shard: 2
    tp: 2
    pp: 2
    cp: 1
    ep: 4
  communication_library:
    name: NCCL
    version: 2.18.5
    env:
      NCCL_IB_QPS_PER_CONNECTION: '4'
      NCCL_DEBUG: INFO
  protocol_selection:
    - RoCEv2  # Inter-node via Slingshot (FSDP/EP cross-node communication)
    - p2p  # Intra-node GPUDirect/NVLink (TP/PP communication)

metric_source:
  traces: [nsys, torch_et, kineto_trace]
  metrics_specific_trace:
    - nvtx_ranges  # For parallelism classification (FSDP/TP/PP/EP)
