# workload_template.yaml
version: 1

description: >  
  Online serving of DeepSeek-V2-Lite on 4 A100 GPUs with EP=4.
  allgather_reducescatter kernel is used as AlltoAll backend.
  Script: https://github.com/sydpeng1/final/blob/main/run_vllm.sh


hf_url: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/tree/main
trace_url: # trace url, e.g., https://drive.google.com/file/d/1EK_YROAho2sBvYVn8idF6DbnaG3SZ0he/view

workload:
  model:
    phase: inference
    moe: true  
    granularity: layer  # Granularity level, e.g., "model_fwd_bwd_pass", "model_fwd" "layer", "kernel".
    model_family: deepseek_v2
    precision: bf16
  data:
    batch_size: 1
    seq_len: 2048
    dataset: wikitext
  hardware:
    network_topo:
      topology: slingshot
      bandwidth_gbps:
        - 200  
    xpu_spec:
      type: GPU 
      model: nvidia_a100
      total_count: 4  
      count_per_node: 4  
    driver_version: cuda_12.4  
  
Model-executor:
  framework: 
    name: vllm
    compiler_tool_selection: 
  model_plan_parallelization:
    dp_replicate: 4
    dp_shard: 1
    tp: 1
    pp: 1
    cp: 1
  communication_library:
    name: NCCL
    version: 2.27.3
    env:
      NCCL_IB_QPS_PER_CONNECTION:  # NCCL environment variable, e.g., 8, 16.
  protocol_selection:
    - rocev2 # Communication protocol for scale-out, e.g., "tcp", "rocev2".
    - p2p # Another communication protocol for scake-up, e.g., "p2p", "memcpy".

metric_source:
  traces:
    -  nsys
  metrics_specific_trace:
    -  # metric specific trace type, e.g., "memory_trace", "accuracy_trace".